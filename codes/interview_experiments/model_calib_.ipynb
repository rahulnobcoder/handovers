{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\internship\\interview_module\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "model_id = \"openai/whisper-small\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speech_calib\\\\jishnu\\\\1.opus',\n",
       " 'speech_calib\\\\rashmi_mp3\\\\1.mp3',\n",
       " 'speech_calib\\\\speaker4\\\\sentence_1.mp3',\n",
       " 'speech_calib\\\\jishnu\\\\2.opus',\n",
       " 'speech_calib\\\\rashmi_mp3\\\\2.mp3',\n",
       " 'speech_calib\\\\speaker4\\\\sentence_2.mp3',\n",
       " 'speech_calib\\\\jishnu\\\\3.opus',\n",
       " 'speech_calib\\\\rashmi_mp3\\\\3.mp3',\n",
       " 'speech_calib\\\\speaker4\\\\sentence_3.mp3',\n",
       " 'speech_calib\\\\jishnu\\\\4.opus',\n",
       " 'speech_calib\\\\rashmi_mp3\\\\4.mp3',\n",
       " 'speech_calib\\\\speaker4\\\\sentence_4.mp3',\n",
       " 'speech_calib\\\\jishnu\\\\5.opus',\n",
       " 'speech_calib\\\\rashmi_mp3\\\\5.mp3',\n",
       " 'speech_calib\\\\speaker4\\\\sentence_5.mp3',\n",
       " 'speech_calib\\\\jishnu\\\\6.opus',\n",
       " 'speech_calib\\\\rashmi_mp3\\\\6.mp3',\n",
       " 'speech_calib\\\\speaker4\\\\sentence_6.mp3',\n",
       " 'speech_calib\\\\jishnu\\\\7.opus',\n",
       " 'speech_calib\\\\rashmi_mp3\\\\7.mp3',\n",
       " 'speech_calib\\\\speaker4\\\\sentence_7.mp3',\n",
       " 'speech_calib\\\\jishnu\\\\8.opus',\n",
       " 'speech_calib\\\\rashmi_mp3\\\\8.mp3',\n",
       " 'speech_calib\\\\speaker4\\\\sentence_8.mp3',\n",
       " 'speech_calib\\\\jishnu\\\\9.opus',\n",
       " 'speech_calib\\\\rashmi_mp3\\\\9.mp3',\n",
       " 'speech_calib\\\\speaker4\\\\sentence_9.mp3',\n",
       " 'speech_calib\\\\jishnu\\\\10.opus',\n",
       " 'speech_calib\\\\rashmi_mp3\\\\10.mp3',\n",
       " 'speech_calib\\\\speaker4\\\\sentence_10.mp3']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "def get_all_files(folder_path):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            all_files.append(full_path)\n",
    "    return all_files\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"speech_calib\"  # Replace with your folder path\n",
    "files = get_all_files(folder_path)\n",
    "def extract_number(file_path):\n",
    "    match = re.search(r'(\\d+)\\.(opus|mp3)', file_path)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "# Sort the file paths by the extracted number\n",
    "sorted_file_paths = sorted(files, key=extract_number)\n",
    "sorted_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:26<00:00,  2.90s/it]\n"
     ]
    }
   ],
   "source": [
    "import librosa \n",
    "from tqdm import tqdm \n",
    "ts=[]\n",
    "language_token_id = processor.tokenizer.convert_tokens_to_ids(\"<|en|>\")\n",
    "for file in tqdm(sorted_file_paths):\n",
    "        y,sr=librosa.load(file,sr=16000)\n",
    "        inputs = processor(y, sampling_rate=sr, return_tensors=\"pt\").input_features\n",
    "        inputs = inputs.to(device, dtype=torch_dtype)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "            inputs,\n",
    "            forced_decoder_ids=[[0, language_token_id]]  # [0, ID] forces the language\n",
    "            )\n",
    "            transcript = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            ts.append(transcript.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: {'word_accuracy': 81.81818181818183, 'char_accuracy': 98.12332439678283}\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_stt_accuracy(transcripts, predictions):\n",
    "    \"\"\"\n",
    "    Calculate word-level and character-level accuracy for speech-to-text predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - transcripts (list of str): List of ground truth text (actual transcripts).\n",
    "    - predictions (list of str): List of predicted text from the model.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary with word-level and character-level accuracy.\n",
    "    \"\"\"\n",
    "    assert len(transcripts) == len(predictions), \"Lists must have the same length.\"\n",
    "    \n",
    "    total_word_matches = 0\n",
    "    total_words = 0\n",
    "    total_char_matches = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    for transcript, prediction in zip(transcripts, predictions):\n",
    "        # Word-level accuracy\n",
    "        transcript_words = transcript.split()\n",
    "        prediction_words = prediction.split()\n",
    "        total_word_matches += len(set(transcript_words) & set(prediction_words))\n",
    "        total_words += len(transcript_words)\n",
    "        \n",
    "        # Character-level accuracy\n",
    "        matcher = SequenceMatcher(None, transcript, prediction)\n",
    "        total_char_matches += sum(block.size for block in matcher.get_matching_blocks())\n",
    "        total_chars += len(transcript)\n",
    "    \n",
    "    word_accuracy = (total_word_matches / total_words) * 100 if total_words > 0 else 0\n",
    "    char_accuracy = (total_char_matches / total_chars) * 100 if total_chars > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"word_accuracy\": word_accuracy,\n",
    "        \"char_accuracy\": char_accuracy\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "sentences=[\n",
    "    \"for some people this room might be\",\n",
    "    \"the scariest place on earth\",\n",
    "    \"behind these black curtains are deadly spiders\",\n",
    "    \"hundreds of them\",\n",
    "    \"and what were gonna do is poke them make them angry\",\n",
    "    \"and then suction the venom that appears\",\n",
    "    \"at the end of their really long fangs\",\n",
    "    \"this is about as close as i ever want to get to a funnel\",\n",
    "    \"and were doing it for a very good reason\",\n",
    "    \"this is a funnel web spider\"\n",
    "]\n",
    "transcripts=[]\n",
    "for s in sentences:\n",
    "    for i in range(3):\n",
    "        transcripts.append(s)\n",
    "predictions = ts\n",
    "accuracy = calculate_stt_accuracy(transcripts, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['for some people this room might be',\n",
       "  'for some people this room might be',\n",
       "  'for some people this room might be',\n",
       "  'the scariest place on earth',\n",
       "  'the scariest place on earth',\n",
       "  'the scariest place on earth',\n",
       "  'behind these black curtains are deadly spiders',\n",
       "  'behind these black curtains are deadly spiders',\n",
       "  'behind these black curtains are deadly spiders',\n",
       "  'hundreds of them',\n",
       "  'hundreds of them',\n",
       "  'hundreds of them',\n",
       "  'and what were gonna do is poke them make them angry',\n",
       "  'and what were gonna do is poke them make them angry',\n",
       "  'and what were gonna do is poke them make them angry',\n",
       "  'and then suction the venom that appears',\n",
       "  'and then suction the venom that appears',\n",
       "  'and then suction the venom that appears',\n",
       "  'at the end of their really long fangs',\n",
       "  'at the end of their really long fangs',\n",
       "  'at the end of their really long fangs',\n",
       "  'this is about as close as i ever want to get to a funnel',\n",
       "  'this is about as close as i ever want to get to a funnel',\n",
       "  'this is about as close as i ever want to get to a funnel',\n",
       "  'and were doing it for a very good reason',\n",
       "  'and were doing it for a very good reason',\n",
       "  'and were doing it for a very good reason',\n",
       "  'this is a funnel web spider',\n",
       "  'this is a funnel web spider',\n",
       "  'this is a funnel web spider'],\n",
       " [' for some people this room might be',\n",
       "  ' for some people this room might be.',\n",
       "  ' for some people, this room might be.',\n",
       "  ' the scariest place on earth',\n",
       "  ' the scariest place on earth',\n",
       "  ' the scariest place on earth',\n",
       "  ' behind these black curtains are deadly spiders.',\n",
       "  ' behind these black curtains are deadly spiders.',\n",
       "  ' behind these black curtains are deadly spiders.',\n",
       "  ' hundreds of them',\n",
       "  ' hundreds of them',\n",
       "  ' hundreds of them.',\n",
       "  ' and what are we going to do is make the mangrove',\n",
       "  ' and what we are gonna do is poke them and make them angry.',\n",
       "  ' and what we are gonna do is spoke them, make them angry.',\n",
       "  ' and then suction the venom that appears',\n",
       "  ' and then suction the venom that appears.',\n",
       "  ' and then suction the venom that appears.',\n",
       "  ' at the end of their really long fans',\n",
       "  ' at the end of their rally long fangs',\n",
       "  ' at the end of their really long fans.',\n",
       "  ' this is about as close as i ever want to get to a funnel.',\n",
       "  ' this is about as close as i ever want to go to a funnel.',\n",
       "  ' this is about as close as i ever want to get to a funnel.',\n",
       "  ' and we are doing it for a very good reason.',\n",
       "  ' and we are doing it for a very good reason.',\n",
       "  \" and we're doing it for a very good reason.\",\n",
       "  ' this is a funnel web spider',\n",
       "  ' this is a funnel web spider',\n",
       "  ' this is a funnel web spider.'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
