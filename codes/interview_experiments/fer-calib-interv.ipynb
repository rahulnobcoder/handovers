{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9447205,"sourceType":"datasetVersion","datasetId":5741761},{"sourceId":9450292,"sourceType":"datasetVersion","datasetId":5744112},{"sourceId":9545617,"sourceType":"datasetVersion","datasetId":5815500},{"sourceId":9750547,"sourceType":"datasetVersion","datasetId":5969633}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2 \nfrom collections import Counter\nimport numpy as np \nfrom tqdm import tqdm ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-18T05:59:52.097247Z","iopub.execute_input":"2024-11-18T05:59:52.098163Z","iopub.status.idle":"2024-11-18T05:59:52.102701Z","shell.execute_reply.started":"2024-11-18T05:59:52.098122Z","shell.execute_reply":"2024-11-18T05:59:52.101584Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef read_jpg_files(directory):\n    jpg_files = []\n    \n    # Walk through the directory and subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.jpg') and 'rot' not in file:\n                # Get the full path to the .jpg file\n                file_path = os.path.join(root, file)\n                jpg_files.append(file_path)\n\n    return jpg_files\n\n# Example: Provide the directory you want to search in\ndirectory_path = \"/kaggle/input/facial-emotion-expressions/images/validation\"\ntrainpaths = read_jpg_files(directory_path)\nreal=np.load('/kaggle/input/realnp/real.npy')","metadata":{"execution":{"iopub.status.busy":"2024-11-18T06:02:29.355027Z","iopub.execute_input":"2024-11-18T06:02:29.355825Z","iopub.status.idle":"2024-11-18T06:02:29.743274Z","shell.execute_reply.started":"2024-11-18T06:02:29.355787Z","shell.execute_reply":"2024-11-18T06:02:29.742493Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"labels=[trainpaths[i].split('/')[-2] for i in range(len(trainpaths))]","metadata":{"execution":{"iopub.status.busy":"2024-11-18T06:02:31.229604Z","iopub.execute_input":"2024-11-18T06:02:31.230405Z","iopub.status.idle":"2024-11-18T06:02:31.239774Z","shell.execute_reply.started":"2024-11-18T06:02:31.230351Z","shell.execute_reply":"2024-11-18T06:02:31.238704Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nfrom tqdm import tqdm \nfrom PIL import Image\n\n# Initialize the pipeline\npipe = pipeline(\"image-classification\", model=\"dima806/facial_emotions_image_detection\",device=0)\n\n# List of image paths\nimage_paths = trainpaths\n\n# Process images and classify them\nresults = []\nfor path in tqdm(image_paths):\n    # Load the image\n    image = Image.open(path)\n    \n    # Use the pipeline to classify the image\n    result = pipe(image)\n    \n    # Get the emotion with the maximum score\n    max_emotion = max(result, key=lambda x: x['score'])\n    \n    # Store the result\n    results.append(max_emotion)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T06:02:31.779371Z","iopub.execute_input":"2024-11-18T06:02:31.779776Z","iopub.status.idle":"2024-11-18T06:04:23.819507Z","shell.execute_reply.started":"2024-11-18T06:02:31.779738Z","shell.execute_reply":"2024-11-18T06:04:23.818356Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"100%|██████████| 7066/7066 [01:51<00:00, 63.31it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"preds=[results[i]['label'] for i in range(len(results))]\nCounter(preds),Counter(labels)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T06:04:35.484989Z","iopub.execute_input":"2024-11-18T06:04:35.485663Z","iopub.status.idle":"2024-11-18T06:04:35.496899Z","shell.execute_reply.started":"2024-11-18T06:04:35.485613Z","shell.execute_reply":"2024-11-18T06:04:35.495910Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"(Counter({'happy': 1805,\n          'neutral': 1193,\n          'sad': 1175,\n          'fear': 997,\n          'angry': 951,\n          'surprise': 823,\n          'disgust': 122}),\n Counter({'happy': 1825,\n          'neutral': 1216,\n          'sad': 1139,\n          'fear': 1018,\n          'angry': 960,\n          'surprise': 797,\n          'disgust': 111}))"},"metadata":{}}]},{"cell_type":"code","source":"final_preds=[]\nfor i in range(len(preds)):\n    if preds[i]=='angry':\n        final_preds.append('anger')\n    else:\n        final_preds.append(preds[i])","metadata":{"execution":{"iopub.status.busy":"2024-11-18T06:07:06.666392Z","iopub.execute_input":"2024-11-18T06:07:06.667136Z","iopub.status.idle":"2024-11-18T06:07:06.674314Z","shell.execute_reply.started":"2024-11-18T06:07:06.667096Z","shell.execute_reply":"2024-11-18T06:07:06.673283Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nprint(classification_report(labels,preds))","metadata":{"execution":{"iopub.status.busy":"2024-11-18T06:07:06.962756Z","iopub.execute_input":"2024-11-18T06:07:06.963573Z","iopub.status.idle":"2024-11-18T06:07:07.059682Z","shell.execute_reply.started":"2024-11-18T06:07:06.963531Z","shell.execute_reply":"2024-11-18T06:07:07.058737Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       angry       0.87      0.86      0.87       960\n     disgust       0.91      1.00      0.95       111\n        fear       0.85      0.83      0.84      1018\n       happy       0.95      0.94      0.94      1825\n     neutral       0.86      0.85      0.86      1216\n         sad       0.80      0.83      0.81      1139\n    surprise       0.90      0.93      0.91       797\n\n    accuracy                           0.88      7066\n   macro avg       0.88      0.89      0.88      7066\nweighted avg       0.88      0.88      0.88      7066\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}